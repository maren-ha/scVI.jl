<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Model training · scVI</title><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="index.html"><img src="assets/logo.svg" alt="scVI logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">scVI</a></span></div><form class="docs-search" action="search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="index.html">Getting started</a></li><li><a class="tocitem" href="DataProcessing.html">Data processing</a></li><li><a class="tocitem" href="scVAE.html">The scVAE model</a></li><li><a class="tocitem" href="scLDVAE.html">The scLDVAE model</a></li><li class="is-active"><a class="tocitem" href="Training.html">Model training</a></li><li><a class="tocitem" href="Evaluation.html">Model evaluation</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href="Training.html">Model training</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="Training.html">Model training</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/maren-ha/scVI.jl/blob/origin/main/docs/src/Training.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Model-training"><a class="docs-heading-anchor" href="#Model-training">Model training</a><a id="Model-training-1"></a><a class="docs-heading-anchor-permalink" href="#Model-training" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="scVI.TrainingArgs" href="#scVI.TrainingArgs"><code>scVI.TrainingArgs</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct TrainingArgs</code></pre><p>Struct to store hyperparameters to control and customise the training process of an <code>scVAE</code> model.  Can be constructed using keywords. </p><p><strong><strong>Keyword arguments:</strong></strong></p><ul><li><code>trainsize::Float32=0.9f0</code>: proportion of data to be used for training when using a train-test split for training. Has no effect when <code>train_test_split==false</code>.</li><li><code>train_test_split::Bool=false</code>: whether or not to randomly split the data into training and test set. </li><li><code>batchsize::Int=128</code>: batchsize to be used when partitioning the data into minibatches for training based on stochastic gradient descent </li><li><code>max_epochs::Int=400</code>: number of epochs to train the model </li><li><code>lr::Float64=1e-3</code>: learning rate (=stepsize) of the ADAM optimiser during the stochastic descent optimisation for model training (for details, see <code>?ADAM</code>). </li><li><code>weight_decay::Float32=0.0f0</code>: rate of weight decay to apply in the ADAM optimiser (for details, see <code>?ADAM</code>).</li><li><code>n_steps_kl_warmup::Union{Int, Nothing}=nothing</code>: number of steps (one gradient descent optimiser update for one batch) over which to perform gradual increase (warm-up, annealing) of the weight of the regularising KL-divergence term in the loss function (ensuring the consistency between variational posterior and standard normal prior). Empirically, this improves model inference.</li><li><code>n_epochs_kl_warmup::Union{Int, Nothing}=400</code>: number of epochs (one update for all batches) over which to perform gradual increase (warm-up, annealing) of the weight of the regularising KL-divergence term in the loss function (ensuring the consistency between variational posterior and standard normal prior). Empirically, this improves model inference.</li><li><code>progress::Bool=true</code>: whether or not to print a progress bar and the current value of the loss function to the REPL.</li><li><code>register_losses::Bool=false</code>: whether or not to record the values of the different loss components after each training epoch in the <code>loss_registry</code> of the <code>scVAE</code> model. If <code>true</code>, for each loss component (reconstruction error, KL divergences, total loss), an array will be created in the dictionary with the name of the loss component as key, where after each epoch, the value of the component is saved.</li><li><code>verbose::Bool=false</code>: only kicks in if <code>progress==false</code>: whether or not to print the current epoch and value of the loss function every <code>verbose_freq</code> epoch. </li><li><code>verbose_freq::Int=10</code>: frequency with which to display the current epoch and current value of the loss function (only if <code>progress==false</code> and <code>verbose==true</code>).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/maren-ha/scVI.jl/blob/f2eff26763fcf4a4ab55bb23bfe4a5b34a1eb7db/src/Training.jl#L1-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="scVI.train_model!" href="#scVI.train_model!"><code>scVI.train_model!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">train_model!(m::scVAE, adata::AnnData, training_args::TrainingArgs; batch_key::Symbol=:batch)</code></pre><p>Trains an <code>scVAE</code> model on an <code>AnnData</code> object, where the behaviour is controlled by a <code>TrainingArgs</code> object:  Defines the ADAM SGD optimiser, collects the model parameters, optionally splits data in training and testdata and  initialises a <code>Flux.DataLoader</code> storing the data in the countmatrix of the <code>AnnData</code> object in batches.  Updates the model parameters via stochastic gradient for the specified number of epochs,  optionally prints out progress and current loss values. </p><p><strong>Arguments</strong></p><ul><li><code>m::scVAE</code>: the model to train</li><li><code>adata::AnnData</code>: the data on which to train the model</li><li><code>training_args::TrainingArgs</code>: the training arguments controlling the training behaviour</li><li><code>batch_key::Symbol=:batch</code>: the key in <code>adata.obs</code> on which to split the data in batches for the library encoder.    If <code>m.use_observed_lib_size==true</code>, this argument is ignored.</li><li>layer::Union{String, Nothing}=nothing<code>: the layer in</code>adata.layers<code>on which to train the model.    If</code>m.gene_likelihood ∈ [:gaussian, :bernoulli]`, this argument is mandatory.</li></ul><p><strong>Returns</strong></p><ul><li>the trained <code>scVAE</code> model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/maren-ha/scVI.jl/blob/f2eff26763fcf4a4ab55bb23bfe4a5b34a1eb7db/src/Training.jl#L81-L101">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="scVI.train_supervised_model!" href="#scVI.train_supervised_model!"><code>scVI.train_supervised_model!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">train_supervised_model!(m::scVAE, adata::AnnData, labels::AbstractVecOrMat{S}, training_args::TrainingArgs) where S &lt;: Real</code></pre><p>Trains a <code>scVAE</code> model on an <code>AnnData</code> object, where the latent representation is additionally trained in a supervised way to match the provided <code>labels</code>,  where the behaviour is controlled by a <code>TrainingArgs</code> object: </p><p>Defines the ADAM SGD optimiser, collects the model parameters, optionally splits data in training and testdata and  initialises a <code>Flux.DataLoader</code> storing the data in the countmatrix of the <code>AnnData</code> object and the corresponding <code>labels</code> for the supervision of the latent representation in batches. </p><p>The loss function used is the ELBO with an additional supervised term (can be checked in the function <code>supervised_loss</code> in <code>src/ModelFunctions.jl</code>:  In addition to the <code>scVAE</code> model and the count data, it has as additional input the provided <code>labels</code>, that need to have the same dimension as the latent represenation.  The mean squared error between the latent representation  and the labels is calculated and added to the standard ELBO loss. </p><p>Updates the model parameters via stochastic gradient for the specified number of epochs,  optionally prints out progress and current loss values. </p><p>Returns the trained <code>scVAE</code> model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/maren-ha/scVI.jl/blob/f2eff26763fcf4a4ab55bb23bfe4a5b34a1eb7db/src/Training.jl#L163-L182">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="scVI.train_tSNE_model!" href="#scVI.train_tSNE_model!"><code>scVI.train_tSNE_model!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">train_tSNE_model!(m::scVAE, adata::AnnData, training_args::TrainingArgs; 
    layer::Union{String, Nothing}=nothing, 
    batch_key::Symbol=:batch,
    tsne_weight::Float32=150.0f0,
    perplexity::Number=30.0, 
    cheat_scale::Float32=12.0f0, 
    cheat::Bool=true)</code></pre><p>Train a scVAE model with tSNE loss, i.e., with an additional component that penalizes  the KL divergence between the distributions of high-dimensional pairwise similarities  of data points and the low-dimensional similarities of the latent space embeddings,  calculated analogous to the objective in a standard tSNE model.</p><p><strong>Arguments</strong></p><ul><li><code>m::scVAE</code>: the model to train</li><li><code>adata::AnnData</code>: the data on which to train the model</li><li><code>training_args::TrainingArgs</code>: the training arguments controlling the training behaviour</li><li><code>batch_key::Symbol=:batch</code>: the key in <code>adata.obs</code> on which to split the data in batches for the library encoder.    If <code>m.use_observed_lib_size==true</code>, this argument is ignored.</li><li>layer::Union{String, Nothing}=nothing<code>: the layer in</code>adata.layers<code>on which to train the model.    If</code>m.gene_likelihood ∈ [:gaussian, :bernoulli]`, this argument is mandatory.</li><li><code>tsne_weight::Float32</code>: weight of tSNE loss</li><li><code>perplexity::Number</code>: perplexity of tSNE</li><li><code>cheat_scale::Number</code>: scale of tSNE loss</li><li><code>cheat::Bool</code>: whether to use early exaggeration</li></ul><p><strong>Returns</strong></p><ul><li><code>m::scVAE</code>: trained scVAE model</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/maren-ha/scVI.jl/blob/f2eff26763fcf4a4ab55bb23bfe4a5b34a1eb7db/src/tSNEPenalty.jl#L171-L200">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="scLDVAE.html">« The scLDVAE model</a><a class="docs-footer-nextpage" href="Evaluation.html">Model evaluation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Monday 14 August 2023 08:45">Monday 14 August 2023</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
