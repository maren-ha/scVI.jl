var documenterSearchIndex = {"docs":
[{"location":"scVAE.html#The-scVAE-model","page":"The scVAE model","title":"The scVAE model","text":"","category":"section"},{"location":"scVAE.html#Encoder","page":"The scVAE model","title":"Encoder","text":"","category":"section"},{"location":"scVAE.html","page":"The scVAE model","title":"The scVAE model","text":"The implementation is based on the Python implementation of the  scvi-tools encoder.","category":"page"},{"location":"scVAE.html","page":"The scVAE model","title":"The scVAE model","text":"scEncoder","category":"page"},{"location":"scVAE.html#scVI.scEncoder","page":"The scVAE model","title":"scVI.scEncoder","text":"mutable struct scEncoder\n\nJulia implementation of the encoder of a single-cell VAE model corresponding to the scvi-tools encoder. Collects all information on the encoder parameters and stores the basic encoder and mean and variance encoders.  Can be constructed using keywords. \n\nKeyword arguments\n\nencoder: Flux.Chain of fully connected layers realising the first part of the encoder (before the split in mean and variance). For details, see the source code of FC_layers in src/Utils.\nmean_encoder: Flux.Dense fully connected layer realising the latent mean encoder \nn_input: input dimension = number of genes/features\nn_hidden: number of hidden units to use in each hidden layer \nn_output: output dimension of the encoder = dimension of latent space \nn_layers: number of hidden layers in encoder and decoder \nvar_activation: whether or not to use an activation function for the variance layer in the encoder\nvar_encoder: Flux.Dense fully connected layer realising the latent variance encoder \nvar_eps: numerical stability constant to add to the variance in the reparameterisation of the latent representation\nz_transformation: whether to apply a softmax transformation the latent z if assuming a lognormal instead of a normal distribution\n\n\n\n\n\n","category":"type"},{"location":"scVAE.html","page":"The scVAE model","title":"The scVAE model","text":"scEncoder(\n    n_input::Int, \n    n_output::Int;\n    activation_fn::Function=relu, # to use in FC_layers\n    bias::Bool=true,\n    n_hidden::Int=128,\n    n_layers::Int=1,\n    distribution::Symbol=:normal,\n    dropout_rate::Float32=0.1f0,\n    use_activation::Bool=true,\n    use_batch_norm::Bool=true,\n    use_layer_norm::Bool=false,\n    var_activation=nothing,\n    var_eps::Float32=Float32(1e-4)\n)","category":"page"},{"location":"scVAE.html#scVI.scEncoder-Tuple{Int64, Int64}","page":"The scVAE model","title":"scVI.scEncoder","text":"scEncoder(\n    n_input::Int, \n    n_output::Int;\n    activation_fn::Function=relu, # to use in FC_layers\n    bias::Bool=true,\n    n_hidden::Union{Int,Vector{Int}}=128,\n    n_layers::Int=1,\n    distribution::Symbol=:normal,\n    dropout_rate::Float32=0.1f0,\n    use_activation::Bool=true,\n    use_batch_norm::Bool=true,\n    use_layer_norm::Bool=false,\n    var_activation=nothing,\n    var_eps::Float32=Float32(1e-4)\n)\n\nConstructor for an scVAE encoder. Initialises an scEncoder object according to the input parameters.  Julia implementation of the scvi-tools encoder.\n\nArguments:\n\nn_input: input dimension = number of genes/features\nn_output: output dimension of the encoder = latent space dimension\n\nKeyword arguments:\n\nactivation_fn: function to use as activation in all encoder neural network layers \nbias: whether or not to use bias parameters in the encoder neural network layers\nn_hidden: number of hidden units to use in each hidden layer (if an Int is passed, this number is used in all hidden layers,    alternatively an array of Ints can be passed, in which case the kth element corresponds to the number of units in the kth layer.\nn_layers: number of hidden layers in encoder \ndistribution :whether to use a :normal or lognormal (:ln) distribution for the latent z  \ndropout_rate: dropout to use in all encoder layers. Setting the rate to 0.0 corresponds to no dropout. \nuse_activation: whether or not to use an activation function in the encoder neural network layers; if false, overrides choice in actication_fn\nuse_batch_norm: whether or not to apply batch normalization in the encoder layers\nuse_layer_norm: whether or not to apply layer normalization in the encoder layers\nvar_activation: whether or not to use an activation function for the variance layer in the encoder\nvar_eps: numerical stability constant to add to the variance in the reparameterisation of the latent representation\n\n\n\n\n\n","category":"method"},{"location":"scVAE.html#Decoder","page":"The scVAE model","title":"Decoder","text":"","category":"section"},{"location":"scVAE.html","page":"The scVAE model","title":"The scVAE model","text":"The implementation is based on the Python implementation of the  scvi-tools decoder.","category":"page"},{"location":"scVAE.html","page":"The scVAE model","title":"The scVAE model","text":"scDecoder","category":"page"},{"location":"scVAE.html#scVI.scDecoder","page":"The scVAE model","title":"scVI.scDecoder","text":"mutable struct scDecoder <: AbstractDecoder\n\nJulia implementation of the decoder for a single-cell VAE model corresponding to the scvi-tools decoder. Collects all information on the decoder parameters and stores the decoder parts.  Can be constructed using keywords. \n\nKeyword arguments\n\nn_input: input dimension = dimension of latent space \nn_hidden: number of hidden units to use in each hidden layer (if an Int is passed, this number is used in all hidden layers,   alternatively an array of Ints can be passed, in which case the kth element corresponds to the number of units in the kth layer.\nn_output: output dimension of the decoder = number of genes/features\nn_layers: number of hidden layers in decoder \npx_decoder: Flux.Chain of fully connected layers realising the first part of the decoder (before the split in mean, dispersion and dropout decoder). For details, see the source code of FC_layers in src/Utils.\npx_dropout_decoder: if the generative distribution is zero-inflated negative binomial (gene_likelihood = :zinb in the scVAE model construction): Flux.Dense layer, else nothing.\npx_r_decoder: decoder for the dispersion parameter. If generative distribution is not some (zero-inflated) negative binomial, it is nothing. Else, it is a parameter vector  or a Flux.Dense, depending on whether the dispersion is estimated per gene (dispersion = :gene), or per gene and cell (dispersion = :gene_cell)  \npx_scale_decoder: decoder for the mean of the reconstruction, Flux.Chain of a Dense layer followed by softmax activation\nuse_batch_norm: whether or not to apply batch normalization in the decoder layers\nuse_layer_norm: whether or not to apply layer normalization in the decoder layers \n\n\n\n\n\n","category":"type"},{"location":"scVAE.html","page":"The scVAE model","title":"The scVAE model","text":"scDecoder(n_input, n_output; \n    activation_fn::Function=relu,\n    bias::Bool=true,\n    dispersion::Symbol=:gene,\n    dropout_rate::Float32=0.0f0,\n    gene_likelihood::Symbol=:zinb,\n    n_hidden::Int=128,\n    n_layers::Int=1, \n    use_activation::Bool=true,\n    use_batch_norm::Bool=true,\n    use_layer_norm::Bool=false\n)","category":"page"},{"location":"scVAE.html#scVI.scDecoder-Tuple{Any, Any}","page":"The scVAE model","title":"scVI.scDecoder","text":"scDecoder(n_input, n_output; \n    activation_fn::Function=relu,\n    bias::Bool=true,\n    dispersion::Symbol=:gene,\n    dropout_rate::Float32=0.0f0,\n    gene_likelihood::Symbol=:zinb,\n    n_hidden::Union{Int,Vector{Int}}=128,\n    n_layers::Int=1, \n    use_activation::Bool=true,\n    use_batch_norm::Bool=true,\n    use_layer_norm::Bool=false\n)\n\nConstructor for an scVAE decoder. Initialises an scDecoder object according to the input parameters.  Julia implementation of the scvi-tools decoder.\n\nArguments:\n\nn_input: input dimension of the decoder = latent space dimension\nn_output: output dimension = number of genes/features in the data \n\nKeyword arguments:\n\nactivation_fn: function to use as activation in all decoder neural network layers \nbias: whether or not to use bias parameters in the decoder neural network layers\ndispersion: whether to estimate the dispersion parameter for the (zero-inflated) negative binomial generative distribution per gene (:gene) or per gene and cell (:gene_cell) \ndropout_rate: dropout to use in all decoder layers. Setting the rate to 0.0 corresponds to no dropout. \nn_hidden: number of hidden units to use in each hidden layer (if an Int is passed, this number is used in all hidden layers,    alternatively an array of Ints can be passed, in which case the kth element corresponds to the number of units in the kth layer.\nn_layers: number of hidden layers in decoder \nuse_activation: whether or not to use an activation function in the decoder neural network layers; if false, overrides choice in actication_fn\nuse_batch_norm: whether or not to apply batch normalization in the decoder layers\nuse_layer_norm: whether or not to apply layer normalization in the decoder layers\n\n\n\n\n\n","category":"method"},{"location":"scVAE.html#VAE-model","page":"The scVAE model","title":"VAE model","text":"","category":"section"},{"location":"scVAE.html","page":"The scVAE model","title":"The scVAE model","text":"The implementation is a basic version of the scvi-tools VAE object. ","category":"page"},{"location":"scVAE.html","page":"The scVAE model","title":"The scVAE model","text":"scVAE","category":"page"},{"location":"scVAE.html#scVI.scVAE","page":"The scVAE model","title":"scVI.scVAE","text":"mutable struct scVAE\n\nJulia implementation of the single-cell Variational Autoencoder model corresponding to the scvi-tools VAE object.  Collects all information on the model parameters such as distribution choices and stores the model encoder and decoder.  Can be constructed using keywords. \n\nKeyword arguments\n\nn_input::Ind: input dimension = number of genes/features\nn_batch::Int=0: number of batches in the data \nn_hidden::Int=128: number of hidden units to use in each hidden layer \nn_latent::Int=10: dimension of latent space \nn_layers::Int=1: number of hidden layers in encoder and decoder \ndispersion::Symbol=:gene: can be either :gene or :gene-cell. The Python scvi-tools options :gene-batch and gene-label are planned, but not supported yet. \nis_trained::Bool=false: indicating whether the model has been trained or not\ndropout_rate: Dropout to use in the encoder and decoder layers. Setting the rate to 0.0 corresponds to no dropout. \ngene_likelihood::Symbol=:zinb: which generative distribution to parameterize in the decoder. Can be one of :nb (negative binomial), :zinb (zero-inflated negative binomial), or :poisson (Poisson). \nlatent_distribution::Symbol=:normal: whether or not to log-transform the input data in the encoder (for numerical stability)\nlibrary_log_means::Union{Nothing, Vector{Float32}}: log-transformed means of library size; has to be provided when not using observed library size, but encoding it\nlibrary_log_vars::Union{Nothing, Vector{Float32}}: log-transformed variances of library size; has to be provided when not using observed library size, but encoding it\nlog_variational: whether or not to log-transform the input data in the encoder (for numerical stability)\nloss_registry::Dict=Dict(): dictionary in which to record the values of the different loss components (reconstruction error, KL divergence(s)) during training \nuse_observed_lib_size::Bool=true: whether or not to use the observed library size (if false, library size is calculated by a dedicated encoder)\nz_encoder::scEncoder: Encoder struct of the VAE model for latent representation; see scEncoder\nl_encoder::Union{Nothing, scEncoder}: Encoder struct of the VAE model for the library size (if use_observed_lib_size==false), see scEncoder\ndecoder::AbstractDecoder: Decoder struct of the VAE model; see scDecoder\n\n\n\n\n\n","category":"type"},{"location":"scVAE.html","page":"The scVAE model","title":"The scVAE model","text":"scVAE(n_input::Int;\n    activation_fn::Function=relu, # to be used in all FC_layers instances\n    bias::Symbol=:both, # whether to use bias in all linear layers of all FC instances \n    dispersion::Symbol=:gene,\n    dropout_rate::Float32=0.1f0,\n    gene_likelihood::Symbol=:zinb,\n    latent_distribution::Symbol=:normal,\n    library_log_means=nothing,\n    library_log_vars=nothing,\n    log_variational::Bool=true,\n    n_batch::Int=1,\n    n_hidden::Int=128,\n    n_latent::Int=10,\n    n_layers::Int=1,\n    use_activation::Symbol=:both, \n    use_batch_norm::Symbol=:both,\n    use_layer_norm::Symbol=:none,\n    use_observed_lib_size::Bool=true,\n    var_activation=nothing,\n    var_eps::Float32=Float32(1e-4),\n    seed::Int=1234\n    )","category":"page"},{"location":"scVAE.html#scVI.scVAE-Tuple{Int64}","page":"The scVAE model","title":"scVI.scVAE","text":"scVAE(n_input::Int;\n    activation_fn::Function=relu, # to be used in all FC_layers instances\n    bias::Symbol=:both, # whether to use bias in all linear layers of all FC instances \n    dispersion::Symbol=:gene,\n    dropout_rate::Float32=0.1f0,\n    gene_likelihood::Symbol=:zinb,\n    latent_distribution::Symbol=:normal,\n    library_log_means=nothing,\n    library_log_vars=nothing,\n    log_variational::Bool=true,\n    n_batch::Int=1,\n    n_hidden::Union{Int,Vector{Int}}=128,\n    n_latent::Int=10,\n    n_layers::Int=1,\n    use_activation::Symbol=:both, \n    use_batch_norm::Symbol=:both,\n    use_layer_norm::Symbol=:none,\n    use_observed_lib_size::Bool=true,\n    var_activation=nothing,\n    var_eps::Float32=Float32(1e-4),\n    seed::Int=1234\n)\n\nConstructor for the scVAE model struct. Initialises an scVAE model with the parameters specified in the input arguments.  Basic Julia implementation of the scvi-tools VAE object. \n\nArguments:\n\nn_input: input dimension = number of genes/features\n\nKeyword arguments\n\nactivation_fn: function to use as activation in all neural network layers of encoder and decoder \nbias: whether or not to use bias parameters in the neural network layers of encoder and decoder\ndispersion: can be either :gene or :gene-cell. The Python scvi-tools options :gene-batch and gene-label are planned, but not supported yet. \ndropout_rate: Dropout to use in the encoder and decoder layers. Setting the rate to 0.0 corresponds to no dropout. \ngene_likelihood: which generative distribution to parameterize in the decoder. Can be one of :nb (negative binomial), :zinb (zero-inflated negative binomial), or :poisson (Poisson). \nlibrary_log_means: log-transformed means of library size; has to be provided when not using observed library size, but encoding it\nlibrary_log_vars: log-transformed variances of library size; has to be provided when not using observed library size, but encoding it\nlog_variational: whether or not to log-transform the input data in the encoder (for numerical stability)\nn_batch: number of batches in the data \nn_hidden: number of hidden units to use in each hidden layer (if an Int is passed, this number is used in all hidden layers,    alternatively an array of Ints can be passed, in which case the kth element corresponds to the number of units in the kth layer.\nn_latent: dimension of latent space \nn_layers: number of hidden layers in encoder and decoder \nuse_activation: whether or not to use an activation function in the neural network layers of encoder and decoder; if false, overrides choice in actication_fn\nuse_batch_norm: whether to apply batch normalization in the encoder/decoder layers; can be one of :encoder, :decoder, both, :none\nuse_layer_norm: whether to apply layer normalization in the encoder/decoder layers; can be one of :encoder, :decoder, both, :none\nuse_observed_lib_size: whether or not to use the observed library size (if false, library size is calculated by a dedicated encoder)\nvar_activation: whether or not to use an activation function for the variance layer in the encoder\nvar_eps: numerical stability constant to add to the variance in the reparameterisation of the latent representation\nseed: random seed to use for initialization of model parameters; to ensure reproducibility. \n\n\n\n\n\n","category":"method"},{"location":"scvis.html#The-scvis-model","page":"The scvis model","title":"The scvis model","text":"","category":"section"},{"location":"scvis.html","page":"The scvis model","title":"The scvis model","text":"scVAE model with additional tSNE loss term, based on the scvis model proposed in Ding J, Condon A and Shah SP Interpretable dimensionality reduction of single cell transcriptome data with deep generative models. Nat Commun 9, 2002 (2018).","category":"page"},{"location":"scvis.html","page":"The scvis model","title":"The scvis model","text":"From the abstract: ","category":"page"},{"location":"scvis.html","page":"The scvis model","title":"The scvis model","text":"scvis is a statistical model that captures and visualizes the low-dimensional structures in single-cell gene expression data. It is robust to the number of data points and learns a probabilistic parametric mapping function to add new data points to an existing embedding.","category":"page"},{"location":"scvis.html#The-additional-t-SNE-loss-component","page":"The scvis model","title":"The additional t-SNE loss component","text":"","category":"section"},{"location":"scvis.html","page":"The scvis model","title":"The scvis model","text":"The model is based on adding a t-SNE objective to the VAE loss function (i.e., the ELBO). This component is supposed to help structure the latent representation to look more like t-SNE. ","category":"page"},{"location":"scvis.html","page":"The scvis model","title":"The scvis model","text":"Similar to the usual t-SNE objective, a matrix transition of transition probabilities has to be calculated. For this, individual perplexities have to be calculated. ","category":"page"},{"location":"scvis.html","page":"The scvis model","title":"The scvis model","text":"compute_transition_probs","category":"page"},{"location":"scvis.html#scVI.scvis.compute_transition_probs","page":"The scvis model","title":"scVI.scvis.compute_transition_probs","text":"Computes the transition probabilities between datapoints in X using the perplexity based similarity approach.\n\nParameters:\n\nX: The data matrix with shape obs x vars.\nperplexity: The perplexity value to use for computing the transition probabilities. Default is 30.\n\nReturns:\n\nA matrix of transition probabilities with shape obs x obs, where P[i,j] represents the transition probability from  datapoint i to j.\n\n\n\n\n\n","category":"function"},{"location":"scvis.html","page":"The scvis model","title":"The scvis model","text":"compute_differentiable_transition_probs","category":"page"},{"location":"scvis.html#scVI.scvis.compute_differentiable_transition_probs","page":"The scvis model","title":"scVI.scvis.compute_differentiable_transition_probs","text":"Compute differentiable transition probabilities from a given matrix X.\nParameters:\n-----------\nX : AbstractMatrix{S}\n    The matrix from which to compute the differentiable transition probabilities.\nperplexity : S\n    The perplexity to use for the computation of the transition probabilities.\nReturns:\n--------\nP : matrix of type S\n    The differentiable transition probabilities.\n\n\n\n\n\n","category":"function"},{"location":"scvis.html","page":"The scvis model","title":"The scvis model","text":"Based on these transition probabilities, the t-SNE loss component is calculated and integrated with the standard ELBO. ","category":"page"},{"location":"scvis.html","page":"The scvis model","title":"The scvis model","text":"tsne_repel","category":"page"},{"location":"scvis.html#scVI.scvis.tsne_repel","page":"The scvis model","title":"scVI.scvis.tsne_repel","text":"Compute the t-SNE cost function with repulsion term.\n\nArgs:\n\nz: matrix of shape (latent_dim, batchsize) representing the low-dimensional   embedding of the data.\nP: matrix of shape (batchsize, batchsize) representing the symmetric    pairwise affinities of the data points.\n\nReturns:\n\nThe t-SNE cost function value normalized by the batchsize.\n\n\n\n\n\n","category":"function"},{"location":"scvis.html","page":"The scvis model","title":"The scvis model","text":"tsne_loss","category":"page"},{"location":"scvis.html#scVI.scvis.tsne_loss","page":"The scvis model","title":"scVI.scvis.tsne_loss","text":"Computes the t-SNE loss for the given scVAE model, tsne network, input data, and transition probabilities.\n\nParameters:     - m: scVAE model     - tsne_net: Dense network for t-SNE     - x: Input data, shape obs x vars      - P: Transition probabilities, shape obs x obs, defaults to Nothing     - epoch: Current training epoch\n\nReturns:     - kl_qp: t-SNE loss\n\n\n\n\n\n","category":"function"},{"location":"scvis.html","page":"The scvis model","title":"The scvis model","text":"scvis_loss","category":"page"},{"location":"scvis.html#scVI.scvis.scvis_loss","page":"The scvis model","title":"scVI.scvis.scvis_loss","text":"Computes the loss value of the scVI model.\n\nIt takes in an scVAE model, a matrix of input data (x), and an optional matrix of transition probabilities (P).\nIt applies the encoder to the input data to obtain a set of parameters for the approximate posterior of the latent variable (z).\nIt then uses these parameters to perform the reparameterization trick to sample from the approximate posterior.\nIt then applies the generative model to the sampled latent variable to obtain the parameters of the likelihood.\nIt then computes the KL divergence between the approximate posterior and the prior, and the reconstruction loss between the input data and the likelihood.\nIt also computes the tsne_repel loss which is the repulsion loss between the latent representations of the data.\nFinally, it returns the sum of reconstruction loss, KL divergence, and tsne_repel loss normalized by the batch size.\n\nParameters:\n- m: an instance of the scVAE model\n- x: the input data, a matrix of shape obs x vars\n- P: (Optional) the transition probabilities matrix, defaults to nothing\n- kl_weight: (Optional) weight for the KL divergence term, defaults to 1.0\n- epoch: (Optional) current training epoch, defaults to 1\nReturns:\n- lossval: the loss value\n\nParameters:\n- m: an instance of the scVAE model\n- x: input data of shape obs x vars\n- P: (Optional) matrix of transition probabilities. If not provided, it will be computed internally.\n- kl_weight: weighting factor for KL divergence. \n- epoch: The current epoch number.\n\nReturns:\n- lossval: The final loss value\n\n\n\n\n\n","category":"function"},{"location":"scvis.html","page":"The scvis model","title":"The scvis model","text":"There are different versions how this can be used to train scvis models. ","category":"page"},{"location":"scvis.html","page":"The scvis model","title":"The scvis model","text":"train_scvis_model!","category":"page"},{"location":"scvis.html#scVI.scvis.train_scvis_model!","page":"The scvis model","title":"scVI.scvis.train_scvis_model!","text":"Trains an scVAE model with the given AnnData, TrainingArgs and mode.\n\nParameters:\n\nm::scVAE: the scVAE model to be trained\nadata::AnnData: the AnnData object containing the dataset\ntraining_args::TrainingArgs: the TrainingArgs object containing the training parameters such as batch size, learning rate, etc.\nmode::Symbol: the training mode, can be one of the following:   :alternating   :alternating_diff   :joint   :joint_diff\n\nReturns:\n\nNothing. The model is trained in place.\n\nRaises:\n\nA warning if an unsupported training mode is selected.\n\n\n\n\n\nTrain an scVI model in joint mode.\n\nParameters:\n\nm: scVAE model to be trained\nadata: AnnData object containing the data to be used for training\ntraining_args: TrainingArgs object containing the training hyperparameters, such as weight decay, learning rate, batch size, and maximum number of epochs\n::Val{:joint}: Symbol specifying the mode of training, must be :joint\n\nReturns:\n\nTuple of trained scVAE model, AnnData object\n\n\n\n\n\n","category":"function"},{"location":"Training.html#Model-training","page":"Model training","title":"Model training","text":"","category":"section"},{"location":"Training.html","page":"Model training","title":"Model training","text":"TrainingArgs","category":"page"},{"location":"Training.html#scVI.TrainingArgs","page":"Model training","title":"scVI.TrainingArgs","text":"mutable struct TrainingArgs\n\nStruct to store hyperparameters to control and customise the training process of an scVAE model.  Can be constructed using keywords. \n\nKeyword arguments:\n\ntrainsize::Float32=0.9f0: proportion of data to be used for training when using a train-test split for training. Has no effect when train_test_split==false.\ntrain_test_split::Bool=false: whether or not to randomly split the data into training and test set. \nbatchsize::Int=128: batchsize to be used when partitioning the data into minibatches for training based on stochastic gradient descent \nmax_epochs::Int=400: number of epochs to train the model \nlr::Float64=1e-3: learning rate (=stepsize) of the ADAM optimiser during the stochastic descent optimisation for model training (for details, see ?ADAM). \nweight_decay::Float32=0.0f0: rate of weight decay to apply in the ADAM optimiser (for details, see ?ADAM).\nn_steps_kl_warmup::Union{Int, Nothing}=nothing: number of steps (one gradient descent optimiser update for one batch) over which to perform gradual increase (warm-up, annealing) of the weight of the regularising KL-divergence term in the loss function (ensuring the consistency between variational posterior and standard normal prior). Empirically, this improves model inference.\nn_epochs_kl_warmup::Union{Int, Nothing}=400: number of epochs (one update for all batches) over which to perform gradual increase (warm-up, annealing) of the weight of the regularising KL-divergence term in the loss function (ensuring the consistency between variational posterior and standard normal prior). Empirically, this improves model inference.\nprogress::Bool=true: whether or not to print a progress bar and the current value of the loss function to the REPL.\nregister_losses::Bool=false: whether or not to record the values of the different loss components after each training epoch in the loss_registry of the scVAE model. If true, for each loss component (reconstruction error, KL divergences, total loss), an array will be created in the dictionary with the name of the loss component as key, where after each epoch, the value of the component is saved.\nverbose::Bool=false: only kicks in if progress==false: whether or not to print the current epoch and value of the loss function every verbose_freq epoch. \nverbose_freq::Int=10: frequency with which to display the current epoch and current value of the loss function (only if progress==false and verbose==true).\n\n\n\n\n\n","category":"type"},{"location":"Training.html","page":"Model training","title":"Model training","text":"train_model!","category":"page"},{"location":"Training.html#scVI.train_model!","page":"Model training","title":"scVI.train_model!","text":"train_model!(m::scVAE, adata::AnnData, training_args::TrainingArgs; batch_key::Symbol=:batch)\n\nTrains an scVAE model on an AnnData object, where the behaviour is controlled by a TrainingArgs object:  Defines the ADAM SGD optimiser, collects the model parameters, optionally splits data in training and testdata and  initialises a Flux.DataLoader storing the data in the countmatrix of the AnnData object in batches.  Updates the model parameters via stochastic gradient for the specified number of epochs,  optionally prints out progress and current loss values. \n\nReturns the trained scVAE model.\n\n\n\n\n\n","category":"function"},{"location":"Training.html","page":"Model training","title":"Model training","text":"train_supervised_model!","category":"page"},{"location":"Training.html#scVI.train_supervised_model!","page":"Model training","title":"scVI.train_supervised_model!","text":"train_supervised_model!(m::scVAE, adata::AnnData, labels::AbstractVecOrMat{S}, training_args::TrainingArgs) where S <: Real\n\nTrains a scVAE model on an AnnData object, where the latent representation is additionally trained in a supervised way to match the provided labels,  where the behaviour is controlled by a TrainingArgs object: \n\nDefines the ADAM SGD optimiser, collects the model parameters, optionally splits data in training and testdata and  initialises a Flux.DataLoader storing the data in the countmatrix of the AnnData object and the corresponding labels for the supervision of the latent representation in batches. \n\nThe loss function used is the ELBO with an additional supervised term (can be checked in the function supervised_loss in src/ModelFunctions.jl:  In addition to the scVAE model and the count data, it has as additional input the provided labels, that need to have the same dimension as the latent represenation.  The mean squared error between the latent representation  and the labels is calculated and added to the standard ELBO loss. \n\nUpdates the model parameters via stochastic gradient for the specified number of epochs,  optionally prints out progress and current loss values. \n\nReturns the trained scVAE model.\n\n\n\n\n\n","category":"function"},{"location":"Evaluation.html#Model-evaluation","page":"Model evaluation","title":"Model evaluation","text":"","category":"section"},{"location":"Evaluation.html#Extract-latent-representations","page":"Model evaluation","title":"Extract latent representations","text":"","category":"section"},{"location":"Evaluation.html","page":"Model evaluation","title":"Model evaluation","text":"get_latent_representation","category":"page"},{"location":"Evaluation.html#scVI.get_latent_representation","page":"Model evaluation","title":"scVI.get_latent_representation","text":"get_latent_representation(m::scVAE, countmatrix::Matrix; \n    cellindices=nothing, give_mean::Bool=true\n)\n\nComputes the latent representation of an scVAE model on input count data by applying the scVAE encoder. \n\nReturns the mean (default) or a sample of the latent representation (can be controlled by give_mean keyword argument).\n\nArguments:\n\nm::scVAE: scVAE model from which the encoder is applied to get the latent representation\ncountmatrix::Matrix: matrix of counts (e.g., countmatrix field of an AnnData object), which is to be embedded with the scVAE model encoder. Is assumed to be in a (cell x gene) format.\n\nKeyword arguments:\n\ncellindices=nothing: optional; indices of cells (=rows) on which to subset the countmatrix before embedding it \ngive_mean::Bool=true: optional; if true, returns the mean of the latent representation, else returns a sample. \n\n\n\n\n\n","category":"function"},{"location":"Evaluation.html","page":"Model evaluation","title":"Model evaluation","text":"register_latent_representation!","category":"page"},{"location":"Evaluation.html#scVI.register_latent_representation!","page":"Model evaluation","title":"scVI.register_latent_representation!","text":"register_latent_representation!(adata::AnnData, m::scVAE)\n\nCalculates the latent representation obtained from encoding the countmatrix of the AnnData object  with a trained scVAE model by applying the function get_latent_representation(m, adata.countmatrix).  Stored the latent representation in the scVI_latent field of the input AnnData object. \n\nReturns the modified AnnData object.\n\n\n\n\n\n","category":"function"},{"location":"Evaluation.html","page":"Model evaluation","title":"Model evaluation","text":"get_loadings","category":"page"},{"location":"Evaluation.html#scVI.get_loadings","page":"Model evaluation","title":"scVI.get_loadings","text":"get_loadings(dec::scLinearDecoder)\n\nExtracts the loadings of a scLinearDecoder, specifically corresponding to  the weight matrix of the linear scLinearDecoder.factor_regressor layer.  If batch normalisation is applied, the weight matrix is re-scaled according  to the accumulated statistics in the batch norm layer (for details, see ?Flux.BatchNorm).\n\nReturns the matrix of loadings. \n\n\n\n\n\n","category":"function"},{"location":"Evaluation.html#Dimension-reduction-and-plotting","page":"Model evaluation","title":"Dimension reduction and plotting","text":"","category":"section"},{"location":"Evaluation.html","page":"Model evaluation","title":"Model evaluation","text":"register_umap_on_latent!","category":"page"},{"location":"Evaluation.html#scVI.register_umap_on_latent!","page":"Model evaluation","title":"scVI.register_umap_on_latent!","text":"register_umap_on_latent!(adata::AnnData, m::scVAE)\n\nCalculates a UMAP (Uniform Manifold Projection and Embedding, McInnes et al. 2018) embedding of the latent representation obtained from encoding the countmatrix of the AnnData object  with a trained scVAE model. If a latent representation is already stored in adata.scVI_latent, this is used for calculating  the UMAP, if not, a latent representation is calculated and registered by calling register_latent_representation!(adata, m). \n\nThe UMAP is calculated using the Julia package UMAP.jl with default parameters.  It is then stored in the scVI_latent_umap field of the input AnnData object. \n\nReturns the modified AnnData object.    \n\n\n\n\n\n","category":"function"},{"location":"Evaluation.html","page":"Model evaluation","title":"Model evaluation","text":"plot_umap_on_latent","category":"page"},{"location":"Evaluation.html#scVI.plot_umap_on_latent","page":"Model evaluation","title":"scVI.plot_umap_on_latent","text":"function plot_umap_on_latent(\n    m::scVAE, adata::AnnData; \n    save_plot::Bool=false, \n    seed::Int=987, \n    filename::String=\"UMAP_on_latent.pdf\"\n)\n\nPlots a UMAP embedding of the latent representation obtained from encoding the countmatrix of the AnnData object with the scVAE model.  If no UMAP representation is stored in adata.scVI_latent_umap, it is calculated and registered by calling register_umap_on_latent(adata, m).\n\nBy default, the cells are color-coded according to the celltypes field of the AnnData object. \n\nFor plotting, the VegaLite.jl package is used.\n\nArguments:\n\nm::scVAE: trained scVAE model to use for embedding the data with the model encoder\nadata:AnnData: data to embed with the model; adata.countmatrix is encoded with m\n\nKeyword arguments:\n\nsave_plot::Bool=true: whether or not to save the plot\nfilename::String=\"UMAP_on_latent.pdf: filename under which to save the plot. Has no effect if save_plot==false.\nseed::Int=987: which random seed to use for calculating UMAP (to ensure reproducibility)\n\n\n\n\n\n","category":"function"},{"location":"Evaluation.html","page":"Model evaluation","title":"Model evaluation","text":"plot_pca_on_latent","category":"page"},{"location":"Evaluation.html#scVI.plot_pca_on_latent","page":"Model evaluation","title":"scVI.plot_pca_on_latent","text":"plot_pca_on_latent(\n    m::scVAE, adata::AnnData; \n    save_plot::Bool=false, \n    filename::String=\"PCA_on_latent.pdf\"\n)\n\nPlots a PCA embedding of the latent representation obtained from encoding the countmatrix of the AnnData object with the scVAE model.  If no latent representation is stored in adata.scVI_latent, it is calculated and registered by calling register_latent_representation(adata, m).\n\nPCA is calculated using the singular value decomposition implementation in LinearAlgebra.jl, see ?LinearAlgebra.svd. For details on the PCA implementation, see the source code in the prcomps function in src/Evaluate.jl.\n\nBy default, the cells are color-coded according to the celltypes field of the AnnData object. \n\n!TODO: add fallback for missing celltype annotation (adata.celltypes = nothing)\n\nFor plotting, the VegaLite.jl package is used.\n\nArguments:\n\nm::scVAE: trained scVAE model to use for embedding the data with the model encoder\nadata:AnnData: data to embed with the model; adata.countmatrix is encoded with m\n\nKeyword arguments:\n\nsave_plot::Bool=true: whether or not to save the plot\nfilename::String=\"UMAP_on_latent.pdf: filename under which to save the plot. Has no effect if save_plot==false.\n\n\n\n\n\n","category":"function"},{"location":"Evaluation.html#Sampling-from-the-trained-model","page":"Model evaluation","title":"Sampling from the trained model","text":"","category":"section"},{"location":"Evaluation.html","page":"Model evaluation","title":"Model evaluation","text":"sample_from_prior","category":"page"},{"location":"Evaluation.html#scVI.sample_from_prior","page":"Model evaluation","title":"scVI.sample_from_prior","text":"sample_from_prior(m::scVAE, adata::AnnData, n_samples::Int; sample_library_size::Bool=false)\n\nSamples from the prior N(0,1) distribution of the latent representation of a trained scVAE model.  Calculates the library size based on the countmatrix of the input AnnData object and either samples from it or uses the mean. Subsequently draws n_samples from the generative distribution defined by the decoder based on the samples from the prior and the library size.\n\nReturns the samples from the model. \n\nArguments:\n\nm::scVAE: trained scVAE model from which to sample\nadata::AnnData: AnnData object based on which to calculate the library size\nn_samples::Int: number of samples to draw\n\nKeyword arguments:\n\nsample_library_size::Bool=false: whether or not to sample from the library size. If false, the mean of the observed library size is used. \n\n\n\n\n\n","category":"function"},{"location":"Evaluation.html","page":"Model evaluation","title":"Model evaluation","text":"sample_from_posterior","category":"page"},{"location":"Evaluation.html#scVI.sample_from_posterior","page":"Model evaluation","title":"scVI.sample_from_posterior","text":"sample_from_posterior(m::scVAE, adata::AnnData)\n\nSamples from the posterior distribution of the latent representation of a trained scVAE model.  Calculates the latent posterior mean and variance and the library size based on the countmatrix of the input AnnData object and samples from the posterior.  Subsequently samples from the generative distribution defined by the decoder based on the samples of the latent representation and the library size. \n\nReturns the samples from the model. \n\nArguments:\n\nm::scVAE: trained scVAE model from which to sample\nadata::AnnData: AnnData object based on which to calculate the latent posterior\n\n\n\n\n\n","category":"function"},{"location":"Evaluation.html","page":"Model evaluation","title":"Model evaluation","text":"Both prior and posterior sampling are based on the following more low-level function, which is not exported but can be called as scVI.decodersample:","category":"page"},{"location":"Evaluation.html","page":"Model evaluation","title":"Model evaluation","text":"decodersample","category":"page"},{"location":"DataProcessing.html#Data-processing","page":"Data processing","title":"Data processing","text":"","category":"section"},{"location":"DataProcessing.html#AnnData-object","page":"Data processing","title":"AnnData object","text":"","category":"section"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"AnnData","category":"page"},{"location":"DataProcessing.html#scVI.AnnData","page":"Data processing","title":"scVI.AnnData","text":"mutable struct AnnData\n\nMinimal Julia implementation of the Python AnnData object (see package documentation and Github repository): An AnnData object stores a countmatrix together with annotations  of observations obs (obsm, obsp), variables var (varm, varp), and unstructured annotations uns.\n\nKeyword arguments\n\ncountmatrix::Matrix: countmatrix in (cell x gene) shape\nlayers::Union{Dict,Nothing}=nothing: dictionary of other layers (e.g., normalized counts) in the same shape as the countmatrix\nobs::Union{DataFrame,Nothing}=nothing: dataframe of information about cells, e.g., celltypes\nobsm::Union{Dict, Nothing}=nothing: dictionary of observation-level matrices, e.g., a UMAP embedding. The first dimension of the matrix has to correspond to the number of cells.\nobsp::Union{Dict, Nothing}=nothing: dictionary of (observation x observation) matrices, e.g., representing cell graphs. \nvar::Union{DataFrame, Nothing}=nothing: dataframe of information about genes/features, e.g., gene names or highly variable genes\nvarm::Union{DataFrame, Nothing}=nothing: dictionary of variable-level matrices. The first dimension of the matrix has to correspond to the number of genes.\nobsp::Union{Dict, Nothing}=nothing: dictionary of (variable x variable) matrices, e.g., representing gene graphs. \ncelltypes=nothing: vector of cell type names, shorthand for adata.obs[\"cell_type\"]\nuns::Union{Dict, Nothing}=nothing: dictionary of unstructured annotation. \n\nExample\n\njulia> adata = load_tasic(\"scvi/data/\")\n    AnnData object with a countmatrix with 1679 cells and 15119 genes\n        layers dict with the following keys: [\"normalized_counts\", \"counts\"]\n        unique celltypes: [\"Vip\", \"L4\", \"L2/3\", \"L2\", \"Pvalb\", \"Ndnf\", \"L5a\", \"SMC\", \"Astro\", \"L5\", \"Micro\", \"Endo\", \"Sst\", \"L6b\", \"Sncg\", \"Igtp\", \"Oligo\", \"Smad3\", \"OPC\", \"L5b\", \"L6a\"]\n\n\n\n\n\n","category":"type"},{"location":"DataProcessing.html#I/O","page":"Data processing","title":"I/O","text":"","category":"section"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"read_h5ad","category":"page"},{"location":"DataProcessing.html#scVI.read_h5ad","page":"Data processing","title":"scVI.read_h5ad","text":"read_h5ad(filename::String)\n\nReads an h5ad file from the given filename, and returns an AnnData object containing the cell-gene expression matrix and other relevant information  stored in the h5ad file.\n\nArguments\n\nfilename::String: Path to the h5ad file.\n\nReturns\n\nadata: An AnnData object containing the following data fields populated \n\nfrom the h5ad file:     - countmatrix: The cell-gene expression matrix.     - layers, obsm, obsp, varm, varp, and uns: Dictionary fields.     - obs and var: DataFrame fields.\n\nExample\n\njulia> adata = read_h5ad(\"mydata.h5ad\")\n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"write_h5ad","category":"page"},{"location":"DataProcessing.html#scVI.write_h5ad","page":"Data processing","title":"scVI.write_h5ad","text":"write_h5ad(adata::AnnData, filename::String)\n\nWrite an AnnData object to an H5AD file.\n\nArguments\n\nadata::AnnData: The AnnData object to write to the H5AD file.\nfilename::String: The path to the H5AD file to write.\n\nReturns\n\nNothing.\n\nExample\n\nadata = read_h5ad(\"example.h5ad\")\nwrite_h5ad(adata, \"output.h5\")\n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html#Library-size-and-normalization","page":"Data processing","title":"Library size and normalization","text":"","category":"section"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"init_library_size","category":"page"},{"location":"DataProcessing.html#scVI.init_library_size","page":"Data processing","title":"scVI.init_library_size","text":"init_library_size(adata::AnnData; batch_key::Symbol=:batch)\n\nComputes and returns library size based on AnnData object. \n\nBased on the scvi-tools function from here \n\nReturns a tupe of arrays of length equal to the number of batches in adata as stored in adata.obs[!,:batch_key],  containing the means and variances of the library size in each batch in adata. Default batch key: :batch, if it is not found, defaults to 1 batch.     \n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"estimate_size_factors","category":"page"},{"location":"DataProcessing.html#scVI.estimate_size_factors","page":"Data processing","title":"scVI.estimate_size_factors","text":"estimate_size_factors(mat; locfunc=median)\n\nEstimates size factors to use for normalization, based on the corresponding Seurat functionality.  Assumes a countmatrix mat in cell x gene format as input, returns a vector of size factors. \n\nFor details, please see the Seurat documentation. \n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"normalize_size_factors","category":"page"},{"location":"DataProcessing.html#scVI.normalize_size_factors","page":"Data processing","title":"scVI.normalize_size_factors","text":"normalize_size_factors(mat::Abstractmatrix)\n\nNormalizes the countdata in mat by dividing it by the size factors calculated with estimate_size_factors.  Assumes a countmatrix mat in cell x gene format as input, returns the normalized matrix.\n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"normalize_size_factors!","category":"page"},{"location":"DataProcessing.html#scVI.normalize_size_factors!","page":"Data processing","title":"scVI.normalize_size_factors!","text":"normalize_size_factors(adata::AnnData)\n\nNormalizes the adata.countmatrix by dividing it by the size factors calculated with estimate_size_factors.  Adds the normalized count matrix to adata.layers and returns adata.\n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"normalize_total!","category":"page"},{"location":"DataProcessing.html#scVI.normalize_total!","page":"Data processing","title":"scVI.normalize_total!","text":"normalize_total!(adata::AnnData; \n    target_sum::Union{Nothing, Real}=nothing, \n    key_added::String=\"cell_counts\",\n    layer::Union{Nothing, String}=nothing,\n    verbose::Bool=false)\n\nNormalizes counts per cell, specifically normalize each cell by total counts over all genes, so that every cell has the same total count after normalization. If choosing target_sum=1e6, this is CPM normalization.\n\nBasic version of the scanpy.pp.normalize_total function\n\nArguments\n\nadata: AnnData object \ntarget_sum: if nothing, after normalization, each observation (cell) has a total count equal to the median of total counts for observations (cells) before normalization.\nkey_added: name of the field in adata.obs where the normalization factor is stored, set to \"cell_counts\" by default \nlayer: optional; which layer to normalize on. If nothing, adata.countmatrix is used. \n\nReturns\n\nReturns adata with normalized version of the original adata.X in adata.layers and the size factors in adata.obs. \n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"normalize_total","category":"page"},{"location":"DataProcessing.html#scVI.normalize_total","page":"Data processing","title":"scVI.normalize_total","text":"normalize_total(adata::AnnData; \n    target_sum::Union{Nothing, Real}=1e4, \n    key_added::String=\"cell_counts\",\n    verbose::Bool=false)\n\nNormalizes counts per cell, specifically normalize each cell by total counts over all genes, so that every cell has the same total count after normalization. See normalize_total! for details.  Unlike the in-place version, this function returns a dictionary with the normalized counts and scaled counts per cell. \n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html#Filtering","page":"Data processing","title":"Filtering","text":"","category":"section"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"filter_cells!","category":"page"},{"location":"DataProcessing.html#scVI.filter_cells!","page":"Data processing","title":"scVI.filter_cells!","text":"function filter_cells!(adata::AnnData; \n    min_counts::Union{Int, Nothing}=nothing, \n    min_genes::Union{Int, Nothing}=nothing, \n    max_counts::Union{Int, Nothing}=nothing, \n    max_genes::Union{Int, Nothing}=nothing,\n    verbose::Bool = true)\n\nFilter cell outliers based on counts and numbers of genes expressed. For instance, only keep cells with at least min_counts counts or min_genes genes expressed. This is to filter measurement outliers, i.e. “unreliable” observations. Only provide one of the optional parameters min_counts, min_genes, max_counts, max_genes per call.\n\nParameters\n\nadata: AnnData object of shape n_obs × n_vars. Rows correspond to cells and columns to genes.\nmin_counts: Minimum number of counts required for a cell to pass filtering.\nmin_genes: Minimum number of genes expressed required for a cell to pass filtering.\nmax_counts: Maximum number of counts required for a cell to pass filtering.\nmax_genes: Maximum number of genes expressed required for a cell to pass filtering.\n\nReturns\n\nThe filtered AnnData object\n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"filter_genes!","category":"page"},{"location":"DataProcessing.html#scVI.filter_genes!","page":"Data processing","title":"scVI.filter_genes!","text":"filter_genes!(adata::AnnData; \n    min_counts::Union{Int, Nothing}=nothing, \n    min_cells::Union{Int, Nothing}=nothing, \n    max_counts::Union{Int, Nothing}=nothing, \n    max_cells::Union{Int, Nothing}=nothing,\n    verbose::Bool = true)\n\nFilter genes based on number of cells or counts. Keep genes that have at least min_counts counts or are expressed in at least min_cells cells or have at most max_counts counts or are expressed in at most max_cells cells. Only provide one of the optional parameters min_counts, min_cells, max_counts, max_cells per call.\n\nParameters\n\nadata: AnnData object of shape n_obs × n_vars. Rows correspond  to cells and columns to genes.\nmin_counts: Minimum number of counts required for a gene to pass filtering.\nmin_cells: Minimum number of cells expressed required for a gene to pass filtering.\nmax_counts: Maximum number of counts required for a gene to pass filtering.\nmax_cells: Maximum number of cells expressed required for a gene to pass filtering.\n\nReturns\n\nThe filtered AnnData object\n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"normalize_total","category":"page"},{"location":"DataProcessing.html#Simple-transformations","page":"Data processing","title":"Simple transformations","text":"","category":"section"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"log_transform!","category":"page"},{"location":"DataProcessing.html#scVI.log_transform!","page":"Data processing","title":"scVI.log_transform!","text":"log_transform!(adata::AnnData; \n    layer::String=\"normalized\",\n    verbose::Bool=false)\n\nLog-transforms the data. Looks for a layer of normalized counts in adata.layers[\"normalized\"].  If the layer is not there, it uses adata.countmatrix.  Returns the adata object with the log-transformed values in a new layer \"log_transformed\". \n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"logp1_transform!","category":"page"},{"location":"DataProcessing.html#scVI.logp1_transform!","page":"Data processing","title":"scVI.logp1_transform!","text":"logp1_transform!(adata::AnnData; \n    layer::Union{String, Nothing}=nothing,\n    verbose::Bool=false)\n\nLog-transforms the (count) data, adding a pseudocount of 1.  Uses the countmatrix in adata.countmatrix by default, but other layers can be passed using the layer keyword.  Returns the adata object with the log-transformed values in a new layer \"logp1_transformed\". \n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"sqrt_transform!","category":"page"},{"location":"DataProcessing.html#scVI.sqrt_transform!","page":"Data processing","title":"scVI.sqrt_transform!","text":"sqrt_transform!(adata::AnnData; \n    layer::String=\"normalized\",\n    verbose::Bool=false)\n\nSqrt-transforms the data. Looks for a layer of normalized counts in adata.layers[\"normalized\"].  If the layer is not there, it uses adata.countmatrix.  Returns the adata object with the sqrt-transformed values in a new layer \"sqrt_transformed\". \n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"rescale!","category":"page"},{"location":"DataProcessing.html#scVI.rescale!","page":"Data processing","title":"scVI.rescale!","text":"rescale!(adata::AnnData; \n    layer::Union{String, Nothing}=nothing,\n    verbose::Bool=false)\n\nRescales the data to zero mean and unit variance in each gene, using the specified layer. If none is provided, it uses adata.countmatrix.  Returns the adata object with the rescales values in a new layer \"rescaled\". \n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html#Dimension-reduction","page":"Data processing","title":"Dimension reduction","text":"","category":"section"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"pca!","category":"page"},{"location":"DataProcessing.html#scVI.pca!","page":"Data processing","title":"scVI.pca!","text":"function pca!(adata::AnnData; \n    layer::String=\"log_transformed\", \n    n_pcs::Int=size(adata.countmatrix,2),\n    verbose::Bool=false\n)\n\nPerforms a PCA on the specified layer of an AnnData object and stores the results in adata.obsm.  Uses all variables of the layer by default, but the number of PCs to be stored can also be specified with the n_pcs keyword.  Returns the modified AnnData object. \n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"umap!","category":"page"},{"location":"DataProcessing.html#scVI.umap!","page":"Data processing","title":"scVI.umap!","text":"function umap!(adata::AnnData; \n    layer::String=\"log_transformed\", \n    use_pca_init::Bool=false, \n    n_pcs::Int=100, \n    verbose::Bool=true, \n    kwargs...)\n\nPerforms UMAP on the specified layer of an AnnData object.  If the layer is not found, the log-transformed normalized counts are calculated and used.  Optionally, UMAP can be run on a PCA representation, the number of PCs can be specified (default=100).  For customizing the behaviour or UMAP, see the keyword arguments of the UMAP.UMAP_ function.  They can all be passed via the kwargs. \n\nThe fields of the resulting UMAP_ struct are stored as follows:      - the UMAP embedding in adata.obsm[\"umap\"],      - the fuzzy simplicial knn graph in adata.obsp[\"fuzzyneighborgraph\"],      - the KNNs of each cell in adata.obsm[\"knns\"],      - the distances of each cell to its KNNs in adata.obsm[\"knn_dists\"]\n\nReturns the modified AnnData object. \n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html#Highly-variable-genes","page":"Data processing","title":"Highly variable genes","text":"","category":"section"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"highly_variable_genes(adata::AnnData; \n    layer::Union{String,Nothing} = nothing,\n    n_top_genes::Int=2000,\n    batch_key::Union{String,Nothing} = nothing,\n    span::Float64=0.3\n    )","category":"page"},{"location":"DataProcessing.html#scVI.highly_variable_genes-Tuple{AnnData}","page":"Data processing","title":"scVI.highly_variable_genes","text":"highly_variable_genes(adata::AnnData;\n    layer::Union{String,Nothing} = nothing,\n    n_top_genes::Int=2000,\n    batch_key::Union{String,Nothing} = nothing,\n    span::Float64=0.3\n    )\n\nComputes highly variable genes according to the workflows on scanpy and Seurat v3 per batch and returns a dictionary with  the information on the joint HVGs. For the in-place version, see highly_variable_genes!\n\nMore specifically, it is the Julia re-implementation of the corresponding  scanpy function For implementation details, please check the scanpy/Seurat documentations or the source code of the  lower-level _highly_variable_genes_seurat_v3 function in this package.  Results are almost identical to the scanpy function. The differences have been traced back to differences in  the local regression for the mean-variance relationship implemented in the Loess.jl package, that differs slightly  from the corresponding Python implementation. \n\nArguments\n\nadata: AnnData object \nlayer: optional; which layer to use for calculating the HVGs. Function assumes this is a layer of counts. If layer is not provided, adata.countmatrix is used. \nn_top_genes: optional; desired number of highly variable genes. Default: 2000. \nbatch_key: optional; key where to look for the batch indices in adata.obs. If not provided, data is treated as one batch. \nspan: span to use in the loess fit for the mean-variance local regression. See the Loess.jl docs for details. \nreplace_hvgs: whether or not to replace the hvg information if there are already hvgs calculated. If false, the new values are added with a \"_1\" suffix. Default:true,\nverbose: whether or not to print info on current status\n\nReturns\n\nReturns a dictionary containing information on the highly variable genes, specifically containing the following keys is added: \n\nhighly_variable: vector of Bools indicating which genes are highly variable\nhighly_variable_rank: rank of the highly variable genes according to (corrected) variance \nmeans: vector with means of each gene\nvariances: vector with variances of each gene \nvariances_norm: normalized variances of each gene \nhighly_variable_nbatches: if there are batches in the dataset, logs the number of batches in which each highly variable gene was actually detected as highly variable. \n\n\n\n\n\n","category":"method"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"highly_variable_genes!(adata::AnnData; \n    layer::Union{String,Nothing} = nothing,\n    n_top_genes::Int=2000,\n    batch_key::Union{String,Nothing} = nothing,\n    span::Float64=0.3\n    )","category":"page"},{"location":"DataProcessing.html#scVI.highly_variable_genes!-Tuple{AnnData}","page":"Data processing","title":"scVI.highly_variable_genes!","text":"highly_variable_genes!(adata::AnnData;\n    layer::Union{String,Nothing} = nothing,\n    n_top_genes::Int=2000,\n    batch_key::Union{String,Nothing} = nothing,\n    span::Float64=0.3,\n    replace_hvgs::Bool=true,\n    verbose::Bool=false\n    )\n\nComputes highly variable genes per batch according to the workflows on scanpy and Seurat v3 in-place.  This is the in-place version that adds an dictionary containing information on the highly variable genes directly  to the adata.var and returns the modified AnnData object.  For details, see the not-in-place version ?highly_variable_genes. \n\n\n\n\n\n","category":"method"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"subset_to_hvg!(adata::AnnData;\n        layer::Union{String,Nothing} = nothing,\n        n_top_genes::Int=2000,\n        batch_key::Union{String,Nothing} = nothing,\n        span::Float64=0.3\n    )","category":"page"},{"location":"DataProcessing.html#scVI.subset_to_hvg!-Tuple{AnnData}","page":"Data processing","title":"scVI.subset_to_hvg!","text":"subset_to_hvg!(adata::AnnData;\n    layer::Union{String,Nothing} = nothing,\n    n_top_genes::Int=2000,\n    batch_key::Union{String,Nothing} = nothing,\n    span::Float64=0.3,\n    verbose::Bool=true\n)\n\nCalculates highly variable genes with highly_variable_genes! and subsets the AnnData object to the calculated HVGs.  For description of input arguments, see highly_variable_genes!\n\nReturns: adata object subset to the calculated HVGs, both in the countmatrix/layer data used for HVG calculation and in the adata.var dictionary.\n\n\n\n\n\n","category":"method"},{"location":"DataProcessing.html#Loading-built-in-datasets","page":"Data processing","title":"Loading built-in datasets","text":"","category":"section"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"There are currently three datasets for which the package supplies built-in convenience functions for loading, processing and creating corresponding AnnData objects. They can be downloaded from this Google Drive data folder. The folder contains all three datasets, namely ","category":"page"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"the cortex data, corresponding to the cortex dataset from the scvi-tools from Zeisel et al. 2015. The original data can be found here and has been processed analogous to the scvi-tools processing\nthe tasic data from Tasic et al. (2016), available at Gene expression Omnibus (GEO) under accession number GSE71585. Preprocessing and additional annotation according to the original manuscript; annotations are available and loaded together with the countmatrix. \nthe pbmc data (PBMC8k) from Zheng et al. 2017, preprocessed according to the Bioconductor workflow.","category":"page"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"I recommend downloading the complete GoogleDrive folder and having it as a subfolder named data in the current working directory. Then, in any Julia script in the parent directory, the functions load_cortex(), load_pbmc() and load_tasic() can be called without arguments, because the default path where these functions look for the respective dataset is exactly that subfolder named data.  ","category":"page"},{"location":"DataProcessing.html#Cortex-data","page":"Data processing","title":"Cortex data","text":"","category":"section"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"load_cortex_from_h5ad","category":"page"},{"location":"DataProcessing.html#scVI.load_cortex_from_h5ad","page":"Data processing","title":"scVI.load_cortex_from_h5ad","text":"load_cortex_from_h5ad(filename::String=\"cortex_anndata.h5ad\")\n\nReads cortex data from an anndata object created and used with the Python scVI and saved as H5AD file, based on the read_h5ad function. \n\nExtracts information to populate a corresponding Julia AnnData object and returns it.\n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"load_cortex","category":"page"},{"location":"DataProcessing.html#scVI.load_cortex","page":"Data processing","title":"scVI.load_cortex","text":"load_cortex(path::String=\"\"; verbose::Bool=false)\n\nLoads cortex dataset from Zeisel et al. 2015 and creates a corresponding AnnData object. \n\nLooks for a file cortex_anndata.h5ad that can be downloaded from this GoogleDrive data folder.  The functions first looks in the folder passed as path (default: assumes files are in a subfolder named data of the current directory, i.e., that the complete GoogleDrive data folder has been downloaded in the current directory), and alternatively downloads the data if is cannot find the file in the given path (see below).\n\nThe file is the h5 export of the Python AnnData object provided as built-in cortex dataset from scvi-tools,  data is from Zeisel et al. 2015.\n\nIf the file is present, the data is loaded from the Python AnnData object and stored in an analogous Julia AnnData object.  This is handled by the functions init_cortex_from_h5ad and load_cortex_from_h5ad. \n\nAlternatively, if the h5ad file is not found in the folder, the data is downloaded directly  from the original authors and  processed analogous to the scvi-tools processing,  and subsequently stored to a Julia AnnData object. This is handled by the function init_cortex_from_url. \n\nReturns the Julia AnnData object.\n\nExample\n\njulia> load_cortex()\n    AnnData object with a countmatrix with 3005 cells and 1200 genes\n    layers dict with the following keys: [\"counts\"]\n    summary statistics dict with the following keys: [\"n_labels\", \"n_vars\", \"n_batch\", \"n_continuous_covs\", \"n_cells\", \"n_proteins\"]\n    unique celltypes: [\"interneurons\", \"pyramidal SS\", \"pyramidal CA1\", \"oligodendrocytes\", \"microglia\", \"endothelial-mural\", \"astrocytes_ependymal\"]\n    training status: not trained\n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html#PBMC-data","page":"Data processing","title":"PBMC data","text":"","category":"section"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"load_pbmc","category":"page"},{"location":"DataProcessing.html#scVI.load_pbmc","page":"Data processing","title":"scVI.load_pbmc","text":"load_pbmc(path::String = \"data/\")\n\nLoads pbmc dataset from Zheng et al. 2017 and creates a corresponding AnnData object.  Specifically, the PBMC8k version is used, preprocessed according to the Bioconductor workflow.\n\nLoads the following files that can be downloaded from this GoogleDrive data folder: \n\nPBMC_counts.csv: countmatrix  \nPBMC_annotation.csv: cell type annotation\n\nFiles are loaded from the folder passed as path (default: assumes files are in a subfolder named data of the current directory, i.e., that the complete GoogleDrive data folder has been downloaded in the current directory.)\n\nFrom these input files, a Julia AnnData object is created. The countmatrix contains information on  cell barcodes and gene names. The gene name and celltype information is stored in the vars and obs  dictionaries of the AnnData object, respectively. \n\nReturns the Julia AnnData object.\n\nExample\n\njulia> load_pbmc()\n    AnnData object with a countmatrix with 7480 cells and 200 genes\n    unique celltypes: [\"B-cells\", \"CD4+ T-cells\", \"Monocytes\", \"CD8+ T-cells\", \"NK cells\", \"NA\", \"HSC\", \"Erythrocytes\"]\n    training status: not trained\n\n\n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html#Tasic-data","page":"Data processing","title":"Tasic data","text":"","category":"section"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"load_tasic","category":"page"},{"location":"DataProcessing.html#scVI.load_tasic","page":"Data processing","title":"scVI.load_tasic","text":"load_tasic(path::String = \"data/\")\n\nLoads tasic dataset based on Tasic et al. (2016) and creates a corresponding AnnData object. \n\nLoads the following files that can be downloaded from this GoogleDrive data folder: \n\nTasic_countmat.txt: countmatrix  \nTasic_celltypes.txt: cell types\nTasic_genenames.txt: gene names \nTasic_receptorandmarkers.txt: List of receptor and marker genes \n\nFiles are loaded from the folder passed as path (default: assumes files are in a subfolder named data of the current directory, i.e., that the complete GoogleDrive data folder has been downloaded in the current directory.\n\nThe original data is available at Gene expression Omnibus (GEO) under accession number GSE71585.  Preprocessing and annotation has been prepared according to the original manuscript. \n\nFrom these input files, a Julia AnnData object is created. The list of receptor and marker genes is used  to annotate cells as neural vs. non-neural, and annotate the neural cells as  GABA- or Glutamatergic. \n\nThese annotations together with the cell type information and the gene names and receptor/marker list are stored in Dictionaries  in the obs and vars fields of the AnnData obejct. \n\nAdditionally, size factors are calculated and used for normalizing the counts.  The normalized counts are stored in an additional layer named normalized_counts.\n\nReturns the Julia AnnData object.\n\nExample\n\njulia> load_tasic()\n    AnnData object with a countmatrix with 1679 cells and 15119 genes\n    layers dict with the following keys: [\"normalized_counts\", \"counts\"]\n    unique celltypes: [\"Vip\", \"L4\", \"L2/3\", \"L2\", \"Pvalb\", \"Ndnf\", \"L5a\", \"SMC\", \"Astro\", \"L5\", \"Micro\", \"Endo\", \"Sst\", \"L6b\", \"Sncg\", \"Igtp\", \"Oligo\", \"Smad3\", \"OPC\", \"L5b\", \"L6a\"]\n    training status: not trained\n\n\n\n\n\n","category":"function"},{"location":"DataProcessing.html","page":"Data processing","title":"Data processing","text":"subset_tasic!","category":"page"},{"location":"DataProcessing.html#scVI.subset_tasic!","page":"Data processing","title":"scVI.subset_tasic!","text":"subset_tasic!(adata::AnnData)\n\nSubsets an input AnnData object initialized from the Tasic data according load_tasic to the neural cells  and the receptor and marker genes provided as annotation. \n\nSpecifically, the count matrix and the normalized count matrix are subset to these cells and genes,  and the dictionaries with information about cells and genes in adata.obs and adata.vars are also subset accordingly. \n\nReturns the modified AnnData object.\n\n\n\n\n\n","category":"function"},{"location":"index.html#scVI.jl","page":"Getting started","title":"scVI.jl","text":"","category":"section"},{"location":"index.html","page":"Getting started","title":"Getting started","text":"(Image: )","category":"page"},{"location":"index.html","page":"Getting started","title":"Getting started","text":"A Julia package for fitting VAEs to single-cell data using count distributions.  Based on the Python implementation in the scvi-tools package. ","category":"page"},{"location":"index.html#Overview","page":"Getting started","title":"Overview","text":"","category":"section"},{"location":"index.html","page":"Getting started","title":"Getting started","text":"The scVI model was first proposed in Lopez R, Regier J, Cole MB et al. Deep generative modeling for single-cell transcriptomics. Nat Methods 15, 1053-1058 (2018). ","category":"page"},{"location":"index.html","page":"Getting started","title":"Getting started","text":"More on the much more extensive Python package ecosystem scvi-tools can be found on the  website and in the corresponding paper Gayoso A, Lopez R, Xing G. et al. A Python library for probabilistic analysis of single-cell omics data. Nat Biotechnol 40, 163–166 (2022). ","category":"page"},{"location":"index.html","page":"Getting started","title":"Getting started","text":"This is the documentation for the Julia version implementing basic functionality, including the following (non-exhausive list): ","category":"page"},{"location":"index.html","page":"Getting started","title":"Getting started","text":"standard and linearly decoded VAE models \nsupport for negative binomial generative distribution with and without zero-inflation \ndifferent ways of specifying the dispersion parameter \nstore data in a (very basic) Julia version of the Python AnnData objects \nseveral built-in datasets \ntraining routines supporting a wide range of customisable hyperparameters ","category":"page"},{"location":"index.html#Installation","page":"Getting started","title":"Installation","text":"","category":"section"},{"location":"index.html","page":"Getting started","title":"Getting started","text":"The package can be downloaded from the Github repo and added with the Julia package manager via ","category":"page"},{"location":"index.html","page":"Getting started","title":"Getting started","text":"julia> ]\npkg > add \"https://github.com/maren-ha/scVI.jl\"","category":"page"},{"location":"index.html","page":"Getting started","title":"Getting started","text":"or alternatively by ","category":"page"},{"location":"index.html","page":"Getting started","title":"Getting started","text":"julia> using Pkg; Pkg.add(url=\"https://github.com/maren-ha/scVI.jl\")","category":"page"},{"location":"index.html#Contents","page":"Getting started","title":"Contents","text":"","category":"section"},{"location":"index.html","page":"Getting started","title":"Getting started","text":"Pages = [\n    \"DataProcessing.md\", \n    \"scVAE.md\",\n    \"scLDVAE.md\", \n    \"ModelFunctions.md\", \n    \"Training.md\", \n    \"Evaluation.md\", \n    \"Utils.md\"\n]","category":"page"},{"location":"scLDVAE.html#The-scLDVAE-model","page":"The scLDVAE model","title":"The scLDVAE model","text":"","category":"section"},{"location":"scLDVAE.html","page":"The scLDVAE model","title":"The scLDVAE model","text":"scVAE model with a linear decoder. The implementation is based on the scvi-tools linearly decoded VAE. According to the scvi-tools authors, this is turn is based on the model proposed in Svensson et al, 2020.","category":"page"},{"location":"scLDVAE.html","page":"The scLDVAE model","title":"The scLDVAE model","text":"scLinearDecoder","category":"page"},{"location":"scLDVAE.html#scVI.scLinearDecoder","page":"The scLDVAE model","title":"scVI.scLinearDecoder","text":"mutable struct scLinearDecoder <: AbstractDecoder\n\nJulia implementation of a linear decoder for a single-cell LDVAE model corresponding to the scvi-tools linear decoder Collects all information on the decoder parameters and stores the decoder parts.  Can be constructed using keywords. \n\nKeyword arguments\n\nn_input: input dimension = dimension of latent space \nn_output: output dimension of the decoder = number of genes/features\nfactor_regressor: Flux.Chain of fully connected layer + optional normalisation realising the first part of the decoder (before the split in mean, dispersion and dropout decoder). For details, see the source code of FC_layers in src/Utils. Only one layer without activation. \npx_dropout_decoder: if the generative distribution is zero-inflated negative binomial (gene_likelihood = :zinb in the scVAE model construction): Flux.Dense layer, else nothing.\npx_r_decoder: decoder for the dispersion parameter. If generative distribution is not some (zero-inflated) negative binomial, it is nothing. Else, it is a parameter vector  or a Flux.Dense, depending on whether the dispersion is estimated per gene (dispersion = :gene), or per gene and cell (dispersion = :gene_cell)  \nuse_batch_norm: whether or not to apply batch normalization in the decoder layers\nuse_layer_norm: whether or not to apply layer normalization in the decoder layers \n\n\n\n\n\n","category":"type"},{"location":"scLDVAE.html","page":"The scLDVAE model","title":"The scLDVAE model","text":"scLinearDecoder(n_input, n_output; \n    bias::Bool=true,\n    dispersion::Symbol=:gene,\n    gene_likelihood::Symbol=:zinb,\n    dropout_rate::Float32=0.0f0,\n    use_batch_norm::Bool=true,\n    use_layer_norm::Bool=false\n    ) ","category":"page"},{"location":"scLDVAE.html#scVI.scLinearDecoder-Tuple{Any, Any}","page":"The scLDVAE model","title":"scVI.scLinearDecoder","text":"scLinearDecoder(n_input, n_output; \n    bias::Bool=true,\n    dispersion::Symbol=:gene,\n    gene_likelihood::Symbol=:zinb,\n    dropout_rate::Float32=0.0f0,\n    use_batch_norm::Bool=true,\n    use_layer_norm::Bool=false\n    )\n\nConstructor for a linear decoder for an scLDVAE model. Initialises a scLinearDecoder struct with the parameters specified by the inputs.  Julia implementation of the scvi-tools linear decoder.\n\nArguments:\n\nn_input: number of input features for the decoder; has to be equal to the latent space dimension.\nn_output: number of features in the final output layer of the decoder, has to be equal to the number of genes in the dataset.\n\nKeyword arguments:\n\nbias: whether or not to use bias parameters in the neural network layers\ndispersion: can be either :gene or :gene-cell. The Python scvi-tools options :gene-batch and gene-label are planned, but not supported yet. \ndropout_rate: Dropout to use in the encoder and decoder layers. Setting the rate to 0.0 corresponds to no dropout. \ngene_likelihood: which generative distribution to parameterize in the decoder. Can be one of :nb (negative binomial), :zinb (zero-inflated negative binomial), or :poisson (Poisson). \nuse_batch_norm: whether or not to apply batch normalization in the encoder/decoder layers\nuse_layer_norm: whether or not to apply layer normalization in the encoder/decoder layers\n\n\n\n\n\n","category":"method"},{"location":"scLDVAE.html","page":"The scLDVAE model","title":"The scLDVAE model","text":"scLDVAE(n_input::Int;\n    activation_fn::Function=relu, # to be used in all FC_layers instances\n    bias::Symbol=:both,  # :both, :none, :encoder, :decoder; whether to use bias in linear layers of all FC instances in encoder/decoder\n    dispersion::Symbol=:gene,\n    dropout_rate::Float32=0.1f0,\n    gene_likelihood::Symbol=:zinb,\n    latent_distribution::Symbol=:normal,\n    library_log_means=nothing,\n    library_log_vars=nothing,\n    log_variational::Bool=true,\n    n_batch::Int=1,\n    n_hidden::Int=128,\n    n_latent::Int=10,\n    n_layers::Int=1,\n    use_activation::Symbol=:both, # :both, :none, :encoder, :decoder\n    use_batch_norm::Symbol=:both, # :both, :none, :encoder, :decoder\n    use_layer_norm::Symbol=:none, # :both, :none, :encoder, :decoder\n    use_observed_lib_size::Bool=true,\n    var_activation=nothing,\n    var_eps::Float32=Float32(1e-4),\n    seed::Int=1234\n    )","category":"page"},{"location":"scLDVAE.html#scVI.scLDVAE-Tuple{Int64}","page":"The scLDVAE model","title":"scVI.scLDVAE","text":"scLDVAE(n_input::Int;\n    activation_fn::Function=relu, # to be used in all FC_layers instances\n    bias::Symbol=:both,  # :both, :none, :encoder, :decoder; whether to use bias in linear layers of all FC instances in encoder/decoder\n    dispersion::Symbol=:gene,\n    dropout_rate::Float32=0.1f0,\n    gene_likelihood::Symbol=:zinb,\n    latent_distribution::Symbol=:normal,\n    library_log_means=nothing,\n    library_log_vars=nothing,\n    log_variational::Bool=true,\n    n_batch::Int=1,\n    n_hidden::Int=128,\n    n_latent::Int=10,\n    n_layers::Int=1,\n    use_activation::Symbol=:both, # :both, :none, :encoder, :decoder\n    use_batch_norm::Symbol=:both, # :both, :none, :encoder, :decoder\n    use_layer_norm::Symbol=:none, # :both, :none, :encoder, :decoder\n    use_observed_lib_size::Bool=true,\n    var_activation=nothing,\n    var_eps::Float32=Float32(1e-4),\n    seed::Int=1234\n)\n\nConstructor for a linearly decoded VAE model. Initialises an scVAE model with a linear decoder with the parameters specified in the input arguments.  Julia implementation of the scvi-tools LDVAE object.  Differs from the scVAE constructor only in that it defines a linear decoder, see scLinearDecoder.\n\nArguments:\n\nn_input: input dimension = number of genes/features\n\nKeyword arguments\n\nactivation_fn: function to use as activation in all neural network layers of encoder and decoder \nbias: whether or not to use bias parameters in the neural network layers of encoder and decoder\ndispersion: can be either :gene or :gene-cell. The Python scvi-tools options :gene-batch and gene-label are planned, but not supported yet. \ndropout_rate: Dropout to use in the encoder and decoder layers. Setting the rate to 0.0 corresponds to no dropout. \ngene_likelihood: which generative distribution to parameterize in the decoder. Can be one of :nb (negative binomial), :zinb (zero-inflated negative binomial), or :poisson (Poisson). \nlibrary_log_means: log-transformed means of library size; has to be provided when not using observed library size, but encoding it\nlibrary_log_vars: log-transformed variances of library size; has to be provided when not using observed library size, but encoding it\nlog_variational: whether or not to log-transform the input data in the encoder (for numerical stability)\nn_batch: number of batches in the data \nn_hidden: number of hidden units to use in each hidden layer \nn_latent: dimension of latent space \nn_layers: number of hidden layers in encoder and decoder \nuse_activation: whether or not to use an activation function in the neural network layers of encoder and decoder; if false, overrides choice in actication_fn\nuse_batch_norm: whether to apply batch normalization in the encoder/decoder layers; can be one of :encoder, :decoder, both, :none\nuse_layer_norm: whether to apply layer normalization in the encoder/decoder layers; can be one of :encoder, :decoder, both, :none\nuse_observed_lib_size: whether or not to use the observed library size (if false, library size is calculated by a dedicated encoder)\nvar_activation: whether or not to use an activation function for the variance layer in the encoder\nvar_eps: numerical stability constant to add to the variance in the reparameterisation of the latent representation\nseed: random seed to use for initialization of model parameters; to ensure reproducibility. \n\n\n\n\n\n","category":"method"}]
}
